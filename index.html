<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Multi-Agent Reinforcement Learning (MARL) – Recent Advances (2018–2024)</title>
    <!-- MathJax for LaTeX rendering -->
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>

  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Academic Project Page</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="FIRST AUTHOR PERSONAL LINK" target="_blank">First Author</a><sup>*</sup>,</span>
                <span class="author-block">
                  <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Second Author</a><sup>*</sup>,</span>
                  <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Third Author</a>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Institution Name<br>   Conferance name and year 2025</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/YOUR REPO HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>




    <h1>Multi-Agent Reinforcement Learning (MARL) – Recent Advances (2018–2024)</h1>

    <section>
        <h2>Multi-Agent RL Overview</h2>
        <p>In multi-agent reinforcement learning, multiple decision-making agents learn and interact in a shared environment. This setting introduces unique challenges beyond single-agent RL, such as non-stationarity (an agent’s environment is changing due to other agents’ learning), credit assignment (attributing a global reward to individual agents’ actions), and partial observability (each agent has limited local view). Traditionally, MARL problems are categorized by the nature of agent interactions – fully cooperative (agents share goals and rewards), fully competitive (zero-sum games), or mixed settings. To handle these challenges, modern MARL algorithms often leverage centralized training with decentralized execution (CTDE): agents are trained with global information (e.g. a centralized critic or joint value function) but execute using only local observations, combining the stability of centralized learning with the scalability of independent action at runtime. Below, we survey major MARL methodologies from the past five years and compare their approaches.</p>
    </section>

    <section>
        <h2>Classification of MARL Methods (2018–2024)</h2>
        
        <h3>Independent vs. Centralized Learning</h3>
        <p>Early MARL approaches extended single-agent RL by training each agent independently (treating other agents as part of the environment). This Independent Learning is simple but suffers from non-stationarity, as independently updating policies can destabilize learning. Modern algorithms mitigate this via centralized learning, where agents share experience or gradients during training. For example, MADDPG (Multi-Agent Deep Deterministic Policy Gradient, 2017) introduced centralized critics for each agent in mixed cooperative-competitive tasks, stabilizing training by using all agents’ observations for critic updates while keeping actors decentralized. Centralized critics (or centralized value functions) effectively turn a multi-agent problem into a single-agent one during training, addressing non-stationarity. However, fully centralized methods can be intractable as agent counts grow, motivating factorized and scalable approaches described next.</p>

        <h3>Value Decomposition for Cooperative MARL</h3>
        <p>In fully cooperative settings (agents maximize a shared team reward), a prominent class of methods learns a joint Q-value \(Q_{\text{tot}}\) that can be factorized into individual agent-action values, enabling decentralized greedy policies while optimizing a global objective. The Individual-Global-Maximum (IGM) principle formalizes this: each agent’s greedy action w.r.t. its local \(Q_i\) should coincide with the global maximum action for \(Q_{\text{tot}}\). Early work Value-Decomposition Networks (VDN, 2017) simply modeled \(Q_{\text{tot}}\) as the sum of individual \(Q_i\). QMIX (2018) generalized this by a monotonic mixing network: \(Q_{\text{tot}}\) is a continuous monotonic function of per-agent values, allowing more flexible credit assignment while guaranteeing IGM consistency. QMIX became a cornerstone MARL algorithm, achieving state-of-the-art on the StarCraft Multi-Agent Challenge (SMAC) benchmark by efficiently coordinating agents in battle. Its strength lies in scalability and ease of training, but the monotonicity constraint limits the joint value representation.</p>
        <p>Subsequent work relaxed these limitations. QTRAN (2019) introduced a different factorization with learned value transformations to satisfy IGM without a fixed network structure. In theory, QTRAN is more general, but in practice, it struggled with optimization difficulties, often underperforming QMIX. Weighted QMIX (WQMIX, 2020) extended QMIX by re-weighting joint action values, effectively reducing the penalty for suboptimal joint actions and expanding the representational class beyond strictly monotonic factorization. Qatten (2020) and GraphMIX (2020) incorporated attention mechanisms and graph neural networks into value mixing, allowing the factorization to account for varying agent importance or communication topology.</p>

        <h3>Advances beyond QMIX</h3>
        <p>These methods address scenarios where agent contributions to \(Q_{\text{tot}}\) are state-dependent (e.g. one agent’s action is critical only in certain states). More recently, QPLEX (2021) introduced a dueling architecture for multi-agent Q-learning: it factorizes the advantage function (deviation from a baseline value) in a duplex manner, which loosens the constraints of IGM and significantly improves representational capacity. QPLEX achieved superior performance on challenging cooperative tasks (e.g. difficult SMAC battle scenarios) by representing complex joint action value landscapes that QMIX/WQMIX could not. Further innovations include ResQ (2022), which adds a residual correction term to any factorization to capture the leftover team reward not explained by individual \(Q_i\), and HAVEN (2023), a hierarchical two-level value decomposition that coordinates agents at multiple scales. Overall, value decomposition methods are highly effective for teamwork scenarios – they offer strong coordination with low communication, but are limited to cooperative tasks with a well-defined shared reward.</p>

        <h3>Policy Gradient and Actor-Critic Methods</h3>
        <p>Another major branch of MARL extends policy gradient algorithms to multi-agent settings. Counterfactual Multi-Agent (COMA, 2017) was an early multi-agent actor-critic that introduced a centralized critic with a counterfactual advantage to address credit assignment. COMA’s critic can compute an advantage for each agent by imagining that agent’s action changed while others fixed, effectively estimating that agent’s contribution. Around the same time, MADDPG (2017) applied DDPG (continuous actor-critic) with centralized critics in mixed cooperative-competitive environments, enabling agents to learn coordinated continuous controls (e.g. multi-robot coordination or predator-prey tasks).</p>
        <p>In recent years, these ideas have converged into multi-agent variants of popular single-agent algorithms. For example, multi-agent PPO (proximal policy optimization) has been used as a strong baseline for both cooperative and competitive games due to its stability – MAPPO (2021) showed that with careful tuning, a single PPO agent controlling all actors (a form of centralized policy) or multiple PPO agents with shared critics can achieve excellent results on SMAC and multi-agent MuJoCo tasks, often outperforming more specialized methods. Newer actor-critic research has also focused on theoretical convergence in multi-agent settings, and on asynchronous approaches: e.g. an Asynchronous Actor-Critic for MARL (NeurIPS 2022) allowed agents to have different update frequencies or time scales, improving learning efficiency in certain tasks.</p>

        <h3>Communication and Coordination Mechanisms</h3>
        <p>Beyond value or policy learning alone, many MARL methods augment agents with communication protocols or shared knowledge to improve coordination. In settings where each agent has partial information, allowing agents to communicate can dramatically boost performance. Approaches like Differentiable Inter-Agent Learning (DIAL, 2016) and CommNet (2016) first demonstrated that end-to-end learning of continuous communication vectors is possible and beneficial. More recent works have incorporated attention and gating into communication: e.g. ATOC (2018) introduced an attentional communicator that decides when agents should communicate, and TarMAC (2019) used targeted communication via attention to specific agents. G2A (2019) (goal-directed communication) and MAAC (2019) (actor-attention-critic) explicitly employed attention in the critic to focus on relevant agents’ states.</p>

        <h3>Learning about Other Agents</h3>
        <p>Another dimension in MARL research is opponent modeling and ad-hoc team play. In competitive or mixed settings, agents may benefit from modeling the policies or intentions of others. Techniques like LOL (Learning with Opponent-Learning Awareness, 2019) let agents gradient-steer opponents by anticipating their learning steps. Similarly, fictitious self-play and population-based training (as used in AlphaStar (2019) for StarCraft) pit agents against evolving pools of past versions or diverse policies.</p>

        <h3>Theoretical and Other Advances</h3>
        <p>In the last few years, we also see MARL intersecting with areas like game theory and mean-field control. When the number of agents grows large (say hundreds of agents), methods based on mean-field approximations (each agent responds to the distribution of others rather than individual states) become useful. For instance, Mean Field MARL (ICML 2018) modeled each agent’s interaction as with an average effect of neighbors, allowing tractable multi-agent solutions in swarm or traffic scenarios.</p>

        <h3>Timeline of Key MARL Contributions (2018–2024)</h3>
        <p>2018: QMIX introduces monotonic value function factorization for cooperative MARL, enabling tractable credit assignment and dominating benchmarks like SMAC. MADDPG (NIPS 2017/2018) demonstrates centralized-critic actor-critic for mixed environments, a template for continuous-action MARL.</p>
        <p>2019: QTRAN proposes a general factorization with transformation for \(Q_{\text{tot}}\), aiming to relax QMIX’s monotonic limits (though with practical challenges). DeepMind’s AlphaStar (Nature 2019) achieves human-grandmaster performance in StarCraft II via league-based MARL – highlighting the power of population self-play and centralized training at scale.</p>
        <p>2020: WQMIX extends QMIX by weighting joint action-values, improving exploration and value approximation on difficult coordination tasks. Attention-based mixing networks (Qatten) and graph-based critics (e.g. DGN) gain traction for better multi-agent coordination. Advances in multi-agent communication (ATOC, TarMAC) and opponent modeling (LOLA) broaden MARL’s scope in complex domains.</p>
        <p>2021: QPLEX (ICLR 2021) introduces a duplex dueling network that factorizes advantages, achieving new state-of-the-art results on cooperative tasks by overcoming representation limitations of earlier factorization methods. FACMAC (NeurIPS 2021) combines value decomposition with actor-critic, enabling CTDE policy gradients for cooperative tasks (blending the benefits of both paradigms).</p>
        <p>2022: ResQ (NeurIPS 2022) adds residual Q-functions to value factorization, boosting performance by correcting approximation error. MARL theory makes strides: e.g. convergence proofs for independent learners in certain games, and provably efficient offline MARL algorithms. MAT (Multi-Agent Transformer) is proposed, showing that a transformer can serve as a powerful agent policy for MARL by treating multi-agent trajectories like language sequences.</p>
        <p>2023: Hierarchical MARL gains attention – HAVEN (AAAI 2023) uses a two-level Q-mixing strategy (intra-level and inter-level) to coordinate many agents with grouping. Mean-field and graph-based MARL methods are refined for large populations (combining mean-field theory with value decomposition in MDQ, 2023).</p>
        <p>2024: MARL is increasingly combined with large models and planners – e.g. using language models to facilitate agent communication or instructions. Issues of scalability (many agents), heterogeneity (different agent types), and safety (MARL in real-world systems) are forefront.</p>
    </section>

    <section>
        <h2>Predictive Maintenance (PdM) in Energy Systems (Batteries)</h2>
        
        <h3>What is Predictive Maintenance?</h3>
        <p>Predictive maintenance (PdM) is a proactive strategy to schedule maintenance of equipment based on its actual condition and predicted failures, rather than on a fixed schedule. The goal is to perform maintenance just in time – before an impending failure causes downtime, but not so early that useful component life is wasted.</p>

        <h3>PdM in Energy and Battery Systems</h3>
        <p>In energy systems, PdM is crucial due to the high cost of failures and the need for reliability. A prime example is lithium-ion battery systems – used in electric vehicles (EVs), grid storage, and consumer electronics – where unexpected failure or rapid degradation can have serious safety and cost implications.</p>
    </section>

    <section>
        <h2>Major PdM Datasets and Environments</h2>
        <h3>NASA C-MAPSS (Turbofan Engine Dataset)</h3>
        <p>A benchmark from NASA’s Prognostics Center, simulating run-to-failure data for jet engines under various conditions. Each engine has multivariate sensor time-series (e.g. pressures, temperatures) for multiple flights until failure; the task is usually to predict RUL at each time step.</p>

        <h3>NASA Randomized Battery Usage Dataset</h3>
        <p>A dataset of lithium-ion battery degradation from NASA, where several 18650 cells (e.g. labeled RW9, RW10, RW11, RW12) were subjected to randomized charge/discharge cycles until failure. It contains time-series of battery metrics (current, voltage, temperature, capacity) under varying operational profiles.</p>
    </section>





<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Lorem ipsum dolor sit amet, consectetur adipiscing elit. Proin ullamcorper tellus sed ante aliquam tempus. Etiam porttitor urna feugiat nibh elementum, et tempor dolor mattis. Donec accumsan enim augue, a vulputate nisi sodales sit amet. Proin bibendum ex eget mauris cursus euismod nec et nibh. Maecenas ac gravida ante, nec cursus dui. Vivamus purus nibh, placerat ac purus eget, sagittis vestibulum metus. Sed vestibulum bibendum lectus gravida commodo. Pellentesque auctor leo vitae sagittis suscipit.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

















  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
