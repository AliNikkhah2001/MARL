<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Multi-Agent Reinforcement Learning (MARL) – Recent Advances (2018–2024)</title>
    <!-- MathJax for LaTeX rendering -->
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>


  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">


  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>

  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>

</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Academic Project Page</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="FIRST AUTHOR PERSONAL LINK" target="_blank">First Author</a><sup>*</sup>,</span>
                <span class="author-block">
                  <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Second Author</a><sup>*</sup>,</span>
                  <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Third Author</a>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Institution Name<br>   Conferance name and year 2025</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/YOUR REPO HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>









<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Multi-Agent Reinforcement Learning (MARL) on PdM</h2>
        <div class="content has-text-justified">

<!-- HERO TITLE ─────────────────────────────────────────────────────────── -->
<h1 class="title is-1 publication-title">
  Multi-Agent Reinforcement Learning for Enhanced Predictive Maintenance in Complex Systems
</h1>

<!-- NEW CONTENT (after the last existing <section> but before </div></section>) -->
<section>
  <h2>Executive Summary</h2>
  <p>
    This report investigates the synergy between Multi-Agent Reinforcement Learning (MARL) and Predictive Maintenance (PdM) for large-scale industrial systems.  Traditional PdM predicts failures on a per-asset basis; MARL instead learns
    <em>decentralised, co-ordinated</em> policies across interacting components or maintenance teams, addressing non-stationarity, credit assignment and system-level optimisation.  Key findings show that MARL can lower downtime, reduce maintenance cost and
    adapt in real-time to evolving plant conditions.  We outline five concrete research plans—ranging from QMIX on the AI4I-2020 dataset to MADDPG-controlled maintenance robots and Transformer-based sensor agents—designed to validate these claims with both
    real and synthetic PdM data.
  </p>
</section>
          <!-- INSERT RIGHT AFTER THE EXECUTIVE-SUMMARY </section> BLOCK -->
<section>
  <h2>2&nbsp;· Deep Dive into Multi-Agent Reinforcement Learning (MARL)</h2>

  <!-- --- 2.1 Fundamentals --- -->
  <h3>2.1 Fundamentals of MARL</h3>
  <p>
    Multi-Agent Reinforcement Learning (MARL) generalises single-agent RL to settings where
    several autonomous agents learn simultaneously in the same environment.  Because each
    agent’s policy evolves over time, every other agent experiences a <em>non-stationary</em>
    world, violating the Markov assumption that underlies most classic RL algorithms.
    This non-stationarity drives core challenges such as (i) multi-agent credit assignment,
    (ii) exploration of a combinatorial joint-action space, (iii) coordination under partial
    observability, (iv) scalability to dozens or hundreds of agents, and (v) equilibrium
    selection in competitive games.
  </p>

  <!-- --- 2.2 Methodology Classes --- -->
  <h3>2.2 Classification of MARL Methodologies</h3>
  <p>
    <strong>Value-based.</strong> Independent Q-Learning (IQL) trains one Q-network per
    agent but struggles with non-stationarity.  Value-Decomposition Networks (VDN)
    assume the global value is the sum of per-agent values; QMIX extends this with a
    monotonic mixing network conditioned on the global state; QPLEX adds a duplex
    duelling mixer to reach the full Individual-Global-Max (IGM) class.<br>
    <strong>Policy-based.</strong> MADDPG introduces a centralised critic for continuous
    control; MAPPO ports PPO’s clipped-objective stability to cooperative teams.<br>
    <strong>Actor-critic.</strong> COMA computes counterfactual advantages to solve
    credit assignment; Actor-Attention-Critic variants embed attention for
    partial-observability.  Transformer-based MAT recasts MARL as sequence modelling,
    excelling in variable-sized agent sets.
  </p>

  <!-- --- 2.3 Comparative Table --- -->
  <h3>2.3 Comparative Analysis of Representative Algorithms</h3>
  <table class="table is-striped is-fullwidth">
    <thead>
      <tr>
        <th>Method</th><th>Core Principle</th><th>Strengths</th><th>Weaknesses</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>IQL</td>
        <td>Independent Q-functions</td>
        <td>Simple; off-policy</td>
        <td>Non-stationary learning signal</td>
      </tr>
      <tr>
        <td>VDN</td>
        <td>Additive value factorisation</td>
        <td>Easy credit assignment</td>
        <td>Limited expressiveness</td>
      </tr>
      <tr>
        <td>QMIX</td>
        <td>Monotonic mixing network</td>
        <td>Scales to SMAC; CTDE-consistent</td>
        <td>Monotonicity constraint</td>
      </tr>
      <tr>
        <td>QPLEX</td>
        <td>Duplex duelling IGM mixer</td>
        <td>Richer joint-value class</td>
        <td>Higher model complexity</td>
      </tr>
      <tr>
        <td>MADDPG</td>
        <td>Central critic, continuous actions</td>
        <td>Handles mixed settings</td>
        <td>Hyper-parameter sensitive</td>
      </tr>
      <tr>
        <td>MAPPO</td>
        <td>PPO with central critic</td>
        <td>Stable, sample-efficient</td>
        <td>On-policy (higher data cost)</td>
      </tr>
      <tr>
        <td>COMA</td>
        <td>Counterfactual advantage</td>
        <td>Elegant credit assignment</td>
        <td>Expensive critic updates</td>
      </tr>
      <tr>
        <td>MAT</td>
        <td>Transformer sequence model</td>
        <td>Handles variable-agent sets</td>
        <td>Heavy compute budget</td>
      </tr>
    </tbody>
  </table>

  <!-- --- 2.4 Timeline --- -->
  <h3>2.4 Timeline of Key Advances (2017 – 2024)</h3>
  <p>
    <strong>2017 – 2018:</strong> MADDPG introduces centralised critics; VDN &amp; QMIX
    pioneer value factorisation.<br>
    <strong>2019:</strong> COMA solves credit assignment; population-based AlphaStar
    showcases large-scale self-play.<br>
    <strong>2020 – 2021:</strong> WQMIX and QPLEX lift representation limits; MAPPO
    demonstrates the raw power of well-tuned PPO in multi-agent settings.<br>
    <strong>2022 – 2024:</strong> Residual &amp; hierarchical mixers (ResQ, HAVEN),
    mean-field extensions, and Transformer-based MAT push scalability and generalisation.
  </p>
</section>

          <!-- INSERT RIGHT AFTER THE MARL SECTION’S </section> -->
<section>
  <h2>3&nbsp;· Comprehensive Understanding of Predictive&nbsp;Maintenance&nbsp;(PdM)</h2>

  <!-- --- 3.1 Definition --- -->
  <h3>3.1 Defining Predictive Maintenance</h3>
  <p>
    Predictive&nbsp;Maintenance (PdM) is a data-driven strategy that forecasts equipment
    failures and schedules interventions exactly when needed—<em>before</em> breakdown but
    <em>after</em> maximum useful life is extracted.  By continuously monitoring sensor
    streams (vibration, temperature, acoustics, lubrication, etc.), PdM improves uptime,
    reduces maintenance cost, and enhances safety versus preventive (time-based) or
    reactive (run-to-failure) policies.
  </p>

  <!-- --- 3.2 Working principles --- -->
  <h3>3.2 Working Principles</h3>
  <p>
    <strong>Data acquisition.</strong> On-board sensors stream high-frequency telemetry to
    an IoT backbone.<br>
    <strong>Condition monitoring.</strong> Baseline deviation, trend analysis, and anomaly
    detection flag incipient faults.<br>
    <strong>Predictive modelling.</strong> Classical ML (e.g.&nbsp;Random Forests,
    SVMs) and deep learning (CNNs, LSTMs, GRUs, auto-encoders) estimate Remaining Useful
    Lifetime (RUL) or failure probability, guiding just-in-time maintenance.
  </p>

  <!-- --- 3.3 Current modelling landscape --- -->
  <h3>3.3 High-Performing Models &amp; Operational Challenges</h3>
  <p>
    Deep sequence models dominate modern PdM benchmarks: LSTMs/GRUs capture long-range
    temporal dependencies; CNNs excel when signals are converted to images
    (e.g.&nbsp;vibration spectrograms); auto-encoders learn reconstruction-based anomaly
    scores.  Obstacles remain—massive historical data requirements, siloed tables with
    mismatched sampling rates, and class imbalance (failures are rare).  Operational
    deployment further demands interpretable predictions and robust data pipelines.
  </p>

  <!-- --- 3.4 Public & synthetic datasets --- -->
  <h3>3.4 Datasets &amp; Benchmarks used in Top-Tier ML Conferences</h3>
  <table class="table is-striped is-fullwidth">
    <thead>
      <tr>
        <th>Dataset</th><th>Source</th><th>Key Features</th>
        <th>Typical Tasks</th><th>Real / Synthetic</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>AI4I&nbsp;2020</td>
        <td>UCI / Kaggle</td>
        <td>10 k rows, 14 sensors, 5 failure modes</td>
        <td>Failure classification, XAI studies</td>
        <td>Synthetic</td>
      </tr>
      <tr>
        <td>CMAPSS (Turbofan)</td>
        <td>NASA PCoE</td>
        <td>Multivariate engine telemetry, run-to-failure</td>
        <td>RUL regression</td>
        <td>Real</td>
      </tr>
      <tr>
        <td>ALPI</td>
        <td>IEEE DataPort</td>
        <td>Packaging-machine alarm logs</td>
        <td>Anomaly / fault prediction</td>
        <td>Real</td>
      </tr>
      <tr>
        <td>Naval Propulsion CBM</td>
        <td>UCI</td>
        <td>Marine engine sensor suite</td>
        <td>Condition monitoring</td>
        <td>Real</td>
      </tr>
      <tr>
        <td>Genesis Demonstrator</td>
        <td>Kaggle</td>
        <td>Sensor data, multi-unit machine</td>
        <td>Failure prediction</td>
        <td>Real</td>
      </tr>
      <tr>
        <td>Gearbox Fault</td>
        <td>Kaggle</td>
        <td>Vibration signals</td>
        <td>Fault diagnosis</td>
        <td>Real</td>
      </tr>
      <tr>
        <td>Hydraulic System</td>
        <td>UCI</td>
        <td>Pressure, temperature, flow sensors</td>
        <td>Condition monitoring</td>
        <td>Real</td>
      </tr>
      <tr>
        <td>Ultrasonic Flowmeter</td>
        <td>UCI</td>
        <td>Flow diagnostics</td>
        <td>Anomaly detection</td>
        <td>Real</td>
      </tr>
    </tbody>
  </table>

  <p>
    Because industrial data are often proprietary, synthetic generators such as
    <code>TSGM</code> have become popular to prototype algorithms under controlled
    failure distributions while respecting privacy constraints.
  </p>
</section>
<!-- INSERT RIGHT AFTER THE PDM SECTION’S </section> -->
<section>
  <h2>4&nbsp;· Exploring the Synergy&nbsp;— Multi-Agent RL for Predictive Maintenance</h2>

  <!-- --- 4.1 Research problems & challenges --- -->
  <h3>4.1 Identifying Research Problems &amp; Challenges</h3>
  <p>
    Traditional PdM treats each asset in isolation, yet industrial plants exhibit strong
    inter-component dependencies: cooling loops affect spindle life, pumps influence
    bearing load, etc.  MARL provides a principled framework to <em>jointly</em> optimise
    maintenance decisions across such coupled assets, be they physical robots, distributed
    sensors, or human technicians.  Key challenges include defining joint state/action
    spaces, designing sparse but informative rewards (downtime ↘, cost ↘, safety ↗),
    coping with long temporal horizons, and ensuring policy interpretability for field
    engineers.
  </p>

  <!-- --- 4.2 How existing MARL methods map to PdM needs --- -->
  <h3>4.2 Leveraging MARL Methodologies for PdM</h3>
  <p>
    <strong>Value-factorisation</strong> (QMIX, QPLEX) enables decentralised agents
    (e.g.&nbsp;sensors) to learn local actions that minimise system-wide risk.  
    <strong>Central-critic actor–critic</strong> algorithms (MADDPG, MAPPO) suit teams of
    maintenance robots needing continuous control under mixed objectives.  
    <strong>Counterfactual methods</strong> (COMA) illuminate which agent’s inspection
    avoided a failure, solving credit assignment.  
    <strong>Communication-learning nets</strong> (CommNet, ATOC) teach agents when &amp;
    what to broadcast, reducing bandwidth while boosting diagnostic accuracy.  
    Finally, <strong>Transformer-based MAT</strong> treats multi-sensor time-series as
    sequences, scaling to variable asset counts and heterogeneous sensors.
  </p>

  <!-- --- 4.3 Five concrete research plans --- -->
  <h3>4.3 Detailed Research Plans</h3>

  <h4>Research Plan 1 · QMIX on AI4I-2020</h4>
  <ul>
    <li><strong>Dataset:</strong> AI4I 2020 synthetic milling dataset (14 sensors, 5 failure modes).</li>
    <li><strong>Agents &amp; Observations:</strong> Treat each key sensor (temp, vibration, torque …) as an agent.</li>
    <li><strong>MARL Model:</strong> QMIX with <code>PyMARL</code>.</li>
    <li><strong>Modifications:</strong> GRU encoder per agent to capture temporal context; focal-loss to rebalance classes.</li>
    <li><strong>Evaluation:</strong> Precision, Recall, F1, AUC vs single-agent NN baseline.</li>
  </ul>

  <h4>Research Plan 2 · MADDPG Simulated Maintenance Robots</h4>
  <ul>
    <li><strong>Environment:</strong> SimPy-based factory with multiple assets; actions = inspect / lubricate / repair.</li>
    <li><strong>Dataset:</strong> Synthetic online simulation (no real logs required).</li>
    <li><strong>MARL Model:</strong> MADDPG (PyTorch RL or TF-Agents).</li>
    <li><strong>Reward:</strong> −(downtime + maintenance cost); +uptime.</li>
    <li><strong>Metrics:</strong> Mean failures ↓, total cost ↓, average uptime ↑ over 10⁶ steps.</li>
  </ul>

  <h4>Research Plan 3 · MAT for Multi-Sensor RUL Prediction</h4>
  <ul>
    <li><strong>Dataset:</strong> Any public multi-sensor time-series (e.g.&nbsp;Hydraulic System).</li>
    <li><strong>Agents:</strong> One per sensor stream; observations are sliding windows.</li>
    <li><strong>MARL Model:</strong> Multi-Agent Transformer (MAT) in PyTorch.</li>
    <li><strong>Adaptation:</strong> Sigmoid head for binary failure; variable sequence lengths.</li>
    <li><strong>Evaluation:</strong> AUROC vs LSTM and ARIMA baselines.</li>
  </ul>

  <h4>Research Plan 4 · VDN on Pump–Motor–Bearing Simulator</h4>
  <ul>
    <li><strong>Data:</strong> Synthetic generator (TSGM) with mutual degradation coupling.</li>
    <li><strong>Agents:</strong> Pump, motor, bearing (3).</li>
    <li><strong>MARL Model:</strong> Value-Decomposition Networks (VDN) via <code>PyMARL</code>.</li>
    <li><strong>Reward:</strong> Shared function of system uptime − maintenance cost.</li>
    <li><strong>Benchmark:</strong> Compare to fixed-interval maintenance policy.</li>
  </ul>

  <h4>Research Plan 5 · CommNet for Sensor-Agent Communication</h4>
  <ul>
    <li><strong>Dataset:</strong> Same as Plan 3 (real) or CMAPSS.</li>
    <li><strong>Agents:</strong> Each sensor embeds readings and communicates via learned channels.</li>
    <li><strong>MARL Model:</strong> CommNet implemented in PyTorch.</li>
    <li><strong>Study:</strong> Ablate communication; measure failure-prediction accuracy.</li>
    <li><strong>Outcome Metric:</strong> Accuracy ↑ and false-alarm rate ↓ vs non-communicative baseline.</li>
  </ul>

</section>













          



          
    <section>
        <h2>Classification of MARL Methods (2018–2024)</h2>
        
        <h3>Independent vs. Centralized Learning</h3>
        <p>Early MARL approaches extended single-agent RL by training each agent independently (treating other agents as part of the environment). This Independent Learning is simple but suffers from non-stationarity, as independently updating policies can destabilize learning. Modern algorithms mitigate this via centralized learning, where agents share experience or gradients during training. For example, MADDPG (Multi-Agent Deep Deterministic Policy Gradient, 2017) introduced centralized critics for each agent in mixed cooperative-competitive tasks, stabilizing training by using all agents’ observations for critic updates while keeping actors decentralized. Centralized critics (or centralized value functions) effectively turn a multi-agent problem into a single-agent one during training, addressing non-stationarity. However, fully centralized methods can be intractable as agent counts grow, motivating factorized and scalable approaches described next.</p>

        <h3>Value Decomposition for Cooperative MARL</h3>
        <p>In fully cooperative settings (agents maximize a shared team reward), a prominent class of methods learns a joint Q-value \(Q_{\text{tot}}\) that can be factorized into individual agent-action values, enabling decentralized greedy policies while optimizing a global objective. The Individual-Global-Maximum (IGM) principle formalizes this: each agent’s greedy action w.r.t. its local \(Q_i\) should coincide with the global maximum action for \(Q_{\text{tot}}\). Early work Value-Decomposition Networks (VDN, 2017) simply modeled \(Q_{\text{tot}}\) as the sum of individual \(Q_i\). QMIX (2018) generalized this by a monotonic mixing network: \(Q_{\text{tot}}\) is a continuous monotonic function of per-agent values, allowing more flexible credit assignment while guaranteeing IGM consistency. QMIX became a cornerstone MARL algorithm, achieving state-of-the-art on the StarCraft Multi-Agent Challenge (SMAC) benchmark by efficiently coordinating agents in battle. Its strength lies in scalability and ease of training, but the monotonicity constraint limits the joint value representation.</p>
        <p>Subsequent work relaxed these limitations. QTRAN (2019) introduced a different factorization with learned value transformations to satisfy IGM without a fixed network structure. In theory, QTRAN is more general, but in practice, it struggled with optimization difficulties, often underperforming QMIX. Weighted QMIX (WQMIX, 2020) extended QMIX by re-weighting joint action values, effectively reducing the penalty for suboptimal joint actions and expanding the representational class beyond strictly monotonic factorization. Qatten (2020) and GraphMIX (2020) incorporated attention mechanisms and graph neural networks into value mixing, allowing the factorization to account for varying agent importance or communication topology.</p>

        <h3>Advances beyond QMIX</h3>
        <p>These methods address scenarios where agent contributions to \(Q_{\text{tot}}\) are state-dependent (e.g. one agent’s action is critical only in certain states). More recently, QPLEX (2021) introduced a dueling architecture for multi-agent Q-learning: it factorizes the advantage function (deviation from a baseline value) in a duplex manner, which loosens the constraints of IGM and significantly improves representational capacity. QPLEX achieved superior performance on challenging cooperative tasks (e.g. difficult SMAC battle scenarios) by representing complex joint action value landscapes that QMIX/WQMIX could not. Further innovations include ResQ (2022), which adds a residual correction term to any factorization to capture the leftover team reward not explained by individual \(Q_i\), and HAVEN (2023), a hierarchical two-level value decomposition that coordinates agents at multiple scales. Overall, value decomposition methods are highly effective for teamwork scenarios – they offer strong coordination with low communication, but are limited to cooperative tasks with a well-defined shared reward.</p>

        <h3>Policy Gradient and Actor-Critic Methods</h3>
        <p>Another major branch of MARL extends policy gradient algorithms to multi-agent settings. Counterfactual Multi-Agent (COMA, 2017) was an early multi-agent actor-critic that introduced a centralized critic with a counterfactual advantage to address credit assignment. COMA’s critic can compute an advantage for each agent by imagining that agent’s action changed while others fixed, effectively estimating that agent’s contribution. Around the same time, MADDPG (2017) applied DDPG (continuous actor-critic) with centralized critics in mixed cooperative-competitive environments, enabling agents to learn coordinated continuous controls (e.g. multi-robot coordination or predator-prey tasks).</p>
        <p>In recent years, these ideas have converged into multi-agent variants of popular single-agent algorithms. For example, multi-agent PPO (proximal policy optimization) has been used as a strong baseline for both cooperative and competitive games due to its stability – MAPPO (2021) showed that with careful tuning, a single PPO agent controlling all actors (a form of centralized policy) or multiple PPO agents with shared critics can achieve excellent results on SMAC and multi-agent MuJoCo tasks, often outperforming more specialized methods. Newer actor-critic research has also focused on theoretical convergence in multi-agent settings, and on asynchronous approaches: e.g. an Asynchronous Actor-Critic for MARL (NeurIPS 2022) allowed agents to have different update frequencies or time scales, improving learning efficiency in certain tasks.</p>

        <h3>Communication and Coordination Mechanisms</h3>
        <p>Beyond value or policy learning alone, many MARL methods augment agents with communication protocols or shared knowledge to improve coordination. In settings where each agent has partial information, allowing agents to communicate can dramatically boost performance. Approaches like Differentiable Inter-Agent Learning (DIAL, 2016) and CommNet (2016) first demonstrated that end-to-end learning of continuous communication vectors is possible and beneficial. More recent works have incorporated attention and gating into communication: e.g. ATOC (2018) introduced an attentional communicator that decides when agents should communicate, and TarMAC (2019) used targeted communication via attention to specific agents. G2A (2019) (goal-directed communication) and MAAC (2019) (actor-attention-critic) explicitly employed attention in the critic to focus on relevant agents’ states.</p>

        <h3>Learning about Other Agents</h3>
        <p>Another dimension in MARL research is opponent modeling and ad-hoc team play. In competitive or mixed settings, agents may benefit from modeling the policies or intentions of others. Techniques like LOL (Learning with Opponent-Learning Awareness, 2019) let agents gradient-steer opponents by anticipating their learning steps. Similarly, fictitious self-play and population-based training (as used in AlphaStar (2019) for StarCraft) pit agents against evolving pools of past versions or diverse policies.</p>

        <h3>Theoretical and Other Advances</h3>
        <p>In the last few years, we also see MARL intersecting with areas like game theory and mean-field control. When the number of agents grows large (say hundreds of agents), methods based on mean-field approximations (each agent responds to the distribution of others rather than individual states) become useful. For instance, Mean Field MARL (ICML 2018) modeled each agent’s interaction as with an average effect of neighbors, allowing tractable multi-agent solutions in swarm or traffic scenarios.</p>

        <h3>Timeline of Key MARL Contributions (2018–2024)</h3>
        <p>2018: QMIX introduces monotonic value function factorization for cooperative MARL, enabling tractable credit assignment and dominating benchmarks like SMAC. MADDPG (NIPS 2017/2018) demonstrates centralized-critic actor-critic for mixed environments, a template for continuous-action MARL.</p>
        <p>2019: QTRAN proposes a general factorization with transformation for \(Q_{\text{tot}}\), aiming to relax QMIX’s monotonic limits (though with practical challenges). DeepMind’s AlphaStar (Nature 2019) achieves human-grandmaster performance in StarCraft II via league-based MARL – highlighting the power of population self-play and centralized training at scale.</p>
        <p>2020: WQMIX extends QMIX by weighting joint action-values, improving exploration and value approximation on difficult coordination tasks. Attention-based mixing networks (Qatten) and graph-based critics (e.g. DGN) gain traction for better multi-agent coordination. Advances in multi-agent communication (ATOC, TarMAC) and opponent modeling (LOLA) broaden MARL’s scope in complex domains.</p>
        <p>2021: QPLEX (ICLR 2021) introduces a duplex dueling network that factorizes advantages, achieving new state-of-the-art results on cooperative tasks by overcoming representation limitations of earlier factorization methods. FACMAC (NeurIPS 2021) combines value decomposition with actor-critic, enabling CTDE policy gradients for cooperative tasks (blending the benefits of both paradigms).</p>
        <p>2022: ResQ (NeurIPS 2022) adds residual Q-functions to value factorization, boosting performance by correcting approximation error. MARL theory makes strides: e.g. convergence proofs for independent learners in certain games, and provably efficient offline MARL algorithms. MAT (Multi-Agent Transformer) is proposed, showing that a transformer can serve as a powerful agent policy for MARL by treating multi-agent trajectories like language sequences.</p>
        <p>2023: Hierarchical MARL gains attention – HAVEN (AAAI 2023) uses a two-level Q-mixing strategy (intra-level and inter-level) to coordinate many agents with grouping. Mean-field and graph-based MARL methods are refined for large populations (combining mean-field theory with value decomposition in MDQ, 2023).</p>
        <p>2024: MARL is increasingly combined with large models and planners – e.g. using language models to facilitate agent communication or instructions. Issues of scalability (many agents), heterogeneity (different agent types), and safety (MARL in real-world systems) are forefront.</p>
    </section>


        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

















  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
